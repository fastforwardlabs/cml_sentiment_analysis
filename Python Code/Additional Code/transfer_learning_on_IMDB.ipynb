{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the steps for using a previously trained sentiment analysis model, on a new data set. The model being used here, has been trained earlier on the [Sentiment140](http://help.sentiment140.com/home) data. For details on building and training a sentiment analysis model, please see the Python code files in this project, namely, <i>01_read_data.py</i>, <i>02_pre-processing_and_model_training.py</i> and <i>3_model_training_on_sentiment140_notebook</i>. The model will then make predictions on the [IMDB Movie Reviews Dataset](https://www.tensorflow.org/datasets/catalog/imdb_reviews) from Tensorflow. This process of using a previously trained network, and customizing it for a given new task, is called <b>Transfer Learning</b>. We'll be demonstrating the following steps in this notebook:\n",
    "\n",
    "1. Download and prepare the data. We have used the same Tokenizer that was used on the Sentiment140 data.\n",
    "\n",
    "\n",
    "2. Build the model:\n",
    "\n",
    "    a) Load the pre-trained base model\n",
    "    \n",
    "    b) Stack the new classification layers on top\n",
    "    \n",
    "    \n",
    "3. Train the new model, evaluate and make predictions.\n",
    "\n",
    "\n",
    "The following work will look fairly standard to anyone having trained machine learning models using python Jupyter notebooks. The CML platform provides a fully capable Jupyter notebook environment that data scientists know and love."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version:  2.2.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow Version: \", tf.__version__)\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import h5py\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the IMDB dataset from TF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IMDB Movie Reviews dataset is a collection of 50,000 highly polar movie reviews, split into training and test sets with 25,000 reviews in each set. This is included as a Tensorflow package, and is easy to doanload and import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'tensorflow_datasets.core.dataset_info.DatasetInfo'>\n"
     ]
    }
   ],
   "source": [
    "print(type(imdb))\n",
    "print(type(info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're reading the data from the IMDB set, and examining what each of the training and test sets look like, in terms of data size, labels etc.. Both the sets have only two unique labels, 0 and 1, which clearly states that the data is built for a binary classification problem. Also, ensure that the data adheres to the UTF-8 encoding, in order to avoid further troubles with Python readers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = imdb['train'], imdb['test']\n",
    "\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(s.numpy().decode('utf8'))\n",
    "    training_labels.append(l.numpy())\n",
    "\n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(s.numpy().decode('utf8'))\n",
    "    testing_labels.append(l.numpy())\n",
    "\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_labels_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels = set(training_labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_labels = set(testing_labels)\n",
    "unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
       " 'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come.\",\n",
       " \"A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.<br /><br />The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.<br /><br />This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_sentences[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're preparing the text data for training the model. <b>Tokenization</b> and <b>padding</b> are done for the review text, but we will be loading the previously saved tokenizer. This is an important step, because if new tokenizers are used, we will end up preocessing the entire corpus everytime, even for classifying a small sentence. By loading the previous tokenizer, we are using the same word index we had created for the Sentiment140 data (please see <i>02_pre-processing_and_model_training.py</i>).\n",
    "\n",
    "Another significant change made to this data is the maximum length. For the sentiment140 data, we had specified the maximum length to be 16, and as the data was composed of tweets, 16 words was a reasonable assumption. In this case, we have movie reviews, which are often descriptive. We have kept this limit to be 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in the previously saved Tokenizer: 690960\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "max_length = 100\n",
    "padding_type='post'\n",
    "trunc_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "\n",
    "with open('../../models/sentiment140_tokenizer.pickle', 'rb') as handle:\n",
    "    loaded_tokenizer = pickle.load(handle)\n",
    "\n",
    "print('Words in the previously saved Tokenizer: ' + str(len(loaded_tokenizer.word_index)))\n",
    "\n",
    "sequences = loaded_tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "word_index = loaded_tokenizer.word_index\n",
    "\n",
    "testing_sequences = loaded_tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 100)\n",
      "(25000, 100)\n"
     ]
    }
   ],
   "source": [
    "print(padded.shape)\n",
    "print(testing_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 1), ('to', 2), ('the', 3), ('a', 4), ('my', 5), ('and', 6), ('you', 7), ('is', 8), ('it', 9), ('in', 10), ('for', 11), ('of', 12), ('on', 13), ('me', 14), ('so', 15), ('have', 16), ('that', 17), ('but', 18), (\"i'm\", 19), ('just', 20)]\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "n_items = take(20, word_index.items())\n",
    "print(n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of an example training sequence: 111\n",
      "Length of the example training sequence after padding: 100\n",
      "Length of an example test sequence: 275\n",
      "Length of the example test sequence after padding: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of an example training sequence: \" + str(len(sequences[1])))\n",
    "print(\"Length of the example training sequence after padding: \" + str(len(padded[1])))\n",
    "print(\"Length of an example test sequence: \" + str(len(testing_sequences[1])))\n",
    "print(\"Length of the example test sequence after padding: \" + str(len(testing_padded[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training and test data are prepared, load the previously trained model. In this example, we are loading the base model, with the pretrained weights, and adding the classification layers on top of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 16, 100)           69096100  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 9, 32)             25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 4, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 69,130,597\n",
      "Trainable params: 34,497\n",
      "Non-trainable params: 69,096,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model = tf.keras.models.load_model('../../models/model_conv1D_LSTM_with_batch_100_epochs.h5')\n",
    "\n",
    "# Show the base model architecture\n",
    "pre_trained_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer output shape:  (None, 32)\n"
     ]
    }
   ],
   "source": [
    "last_layer = pre_trained_model.get_layer('lstm')\n",
    "print('last layer output shape: ', last_layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm/Identity:0\", shape=(None, 32), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "last_output = last_layer.output\n",
    "print(last_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train new model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we are building a new model, using the base model loaded previously. This process will use the representations learned by the base model to extract meaningful features from new data samples. Additionally, we are adding a new classifier, which will be trained from scratch, on top of the pretrained model. This is done only for repurposing the feature maps learned previously. We then train the model for 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_input (InputLayer) [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 16, 100)           69096100  \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 9, 32)             25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 4, 32)             0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 69,131,141\n",
      "Trainable params: 35,041\n",
      "Non-trainable params: 69,096,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "x = layers.Flatten()(last_output)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x) \n",
    "\n",
    "model = Model(pre_trained_model.input, x)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 16) for input Tensor(\"embedding_input:0\", shape=(None, 16), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 16) for input Tensor(\"embedding_input:0\", shape=(None, 16), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 16) for input Tensor(\"embedding_input:0\", shape=(None, 16), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 16) for input Tensor(\"embedding_input:0\", shape=(None, 16), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 16) for input Tensor(\"embedding_input:0\", shape=(None, 16), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 16) for input Tensor(\"embedding_input:0\", shape=(None, 16), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 14s - loss: 0.5968 - acc: 0.6714 - val_loss: 0.4999 - val_acc: 0.7548\n",
      "Epoch 2/50\n",
      "782/782 - 12s - loss: 0.5036 - acc: 0.7592 - val_loss: 0.5068 - val_acc: 0.7390\n",
      "Epoch 3/50\n",
      "782/782 - 12s - loss: 0.4631 - acc: 0.7823 - val_loss: 0.6284 - val_acc: 0.7106\n",
      "Epoch 4/50\n",
      "782/782 - 12s - loss: 0.4374 - acc: 0.7990 - val_loss: 0.5101 - val_acc: 0.7586\n",
      "Epoch 5/50\n",
      "782/782 - 12s - loss: 0.4166 - acc: 0.8100 - val_loss: 0.4255 - val_acc: 0.8010\n",
      "Epoch 6/50\n",
      "782/782 - 12s - loss: 0.3973 - acc: 0.8210 - val_loss: 0.4331 - val_acc: 0.7955\n",
      "Epoch 7/50\n",
      "782/782 - 12s - loss: 0.3840 - acc: 0.8266 - val_loss: 0.4194 - val_acc: 0.8087\n",
      "Epoch 8/50\n",
      "782/782 - 12s - loss: 0.3705 - acc: 0.8360 - val_loss: 0.6431 - val_acc: 0.7449\n",
      "Epoch 9/50\n",
      "782/782 - 12s - loss: 0.3601 - acc: 0.8386 - val_loss: 0.4207 - val_acc: 0.8061\n",
      "Epoch 10/50\n",
      "782/782 - 12s - loss: 0.3420 - acc: 0.8500 - val_loss: 0.4852 - val_acc: 0.7901\n",
      "Epoch 11/50\n",
      "782/782 - 12s - loss: 0.3310 - acc: 0.8554 - val_loss: 0.4378 - val_acc: 0.8092\n",
      "Epoch 12/50\n",
      "782/782 - 12s - loss: 0.3223 - acc: 0.8606 - val_loss: 0.4810 - val_acc: 0.7930\n",
      "Epoch 13/50\n",
      "782/782 - 12s - loss: 0.3088 - acc: 0.8672 - val_loss: 0.4546 - val_acc: 0.8021\n",
      "Epoch 14/50\n",
      "782/782 - 12s - loss: 0.2992 - acc: 0.8742 - val_loss: 0.5188 - val_acc: 0.7761\n",
      "Epoch 15/50\n",
      "782/782 - 12s - loss: 0.2878 - acc: 0.8797 - val_loss: 0.4499 - val_acc: 0.8037\n",
      "Epoch 16/50\n",
      "782/782 - 12s - loss: 0.2829 - acc: 0.8822 - val_loss: 0.5708 - val_acc: 0.7776\n",
      "Epoch 17/50\n",
      "782/782 - 12s - loss: 0.2750 - acc: 0.8840 - val_loss: 0.4843 - val_acc: 0.8027\n",
      "Epoch 18/50\n",
      "782/782 - 12s - loss: 0.2630 - acc: 0.8914 - val_loss: 0.5437 - val_acc: 0.7876\n",
      "Epoch 19/50\n",
      "782/782 - 12s - loss: 0.2552 - acc: 0.8944 - val_loss: 0.6399 - val_acc: 0.7256\n",
      "Epoch 20/50\n",
      "782/782 - 12s - loss: 0.2489 - acc: 0.8980 - val_loss: 0.6446 - val_acc: 0.7700\n",
      "Epoch 21/50\n",
      "782/782 - 12s - loss: 0.2490 - acc: 0.8982 - val_loss: 0.5411 - val_acc: 0.7778\n",
      "Epoch 22/50\n",
      "782/782 - 12s - loss: 0.2340 - acc: 0.9047 - val_loss: 0.4911 - val_acc: 0.7958\n",
      "Epoch 23/50\n",
      "782/782 - 12s - loss: 0.2274 - acc: 0.9084 - val_loss: 0.5463 - val_acc: 0.7865\n",
      "Epoch 24/50\n",
      "782/782 - 12s - loss: 0.2258 - acc: 0.9083 - val_loss: 0.5845 - val_acc: 0.7919\n",
      "Epoch 25/50\n",
      "782/782 - 12s - loss: 0.2168 - acc: 0.9124 - val_loss: 0.5812 - val_acc: 0.7842\n",
      "Epoch 26/50\n",
      "782/782 - 12s - loss: 0.2096 - acc: 0.9148 - val_loss: 0.6488 - val_acc: 0.7900\n",
      "Epoch 27/50\n",
      "782/782 - 12s - loss: 0.2054 - acc: 0.9185 - val_loss: 0.5984 - val_acc: 0.7746\n",
      "Epoch 28/50\n",
      "782/782 - 12s - loss: 0.2003 - acc: 0.9207 - val_loss: 0.5404 - val_acc: 0.7843\n",
      "Epoch 29/50\n",
      "782/782 - 12s - loss: 0.1992 - acc: 0.9206 - val_loss: 0.5476 - val_acc: 0.7913\n",
      "Epoch 30/50\n",
      "782/782 - 12s - loss: 0.1938 - acc: 0.9237 - val_loss: 0.6313 - val_acc: 0.7910\n",
      "Epoch 31/50\n",
      "782/782 - 12s - loss: 0.1903 - acc: 0.9247 - val_loss: 0.6209 - val_acc: 0.7848\n",
      "Epoch 32/50\n",
      "782/782 - 12s - loss: 0.1852 - acc: 0.9267 - val_loss: 0.5405 - val_acc: 0.7952\n",
      "Epoch 33/50\n",
      "782/782 - 12s - loss: 0.1844 - acc: 0.9280 - val_loss: 0.5541 - val_acc: 0.7871\n",
      "Epoch 34/50\n",
      "782/782 - 12s - loss: 0.1784 - acc: 0.9301 - val_loss: 0.6154 - val_acc: 0.7925\n",
      "Epoch 35/50\n",
      "782/782 - 12s - loss: 0.1732 - acc: 0.9328 - val_loss: 0.6014 - val_acc: 0.7912\n",
      "Epoch 36/50\n",
      "782/782 - 12s - loss: 0.1739 - acc: 0.9333 - val_loss: 0.6296 - val_acc: 0.7876\n",
      "Epoch 37/50\n",
      "782/782 - 12s - loss: 0.1682 - acc: 0.9343 - val_loss: 0.6810 - val_acc: 0.7782\n",
      "Epoch 38/50\n",
      "782/782 - 12s - loss: 0.1685 - acc: 0.9345 - val_loss: 0.6455 - val_acc: 0.7821\n",
      "Epoch 39/50\n",
      "782/782 - 12s - loss: 0.1675 - acc: 0.9354 - val_loss: 0.7376 - val_acc: 0.7850\n",
      "Epoch 40/50\n",
      "782/782 - 12s - loss: 0.1640 - acc: 0.9367 - val_loss: 0.6761 - val_acc: 0.7871\n",
      "Epoch 41/50\n",
      "782/782 - 12s - loss: 0.1569 - acc: 0.9385 - val_loss: 0.7552 - val_acc: 0.7901\n",
      "Epoch 42/50\n",
      "782/782 - 12s - loss: 0.1568 - acc: 0.9396 - val_loss: 0.6815 - val_acc: 0.7868\n",
      "Epoch 43/50\n",
      "782/782 - 12s - loss: 0.1548 - acc: 0.9395 - val_loss: 0.6370 - val_acc: 0.7891\n",
      "Epoch 44/50\n",
      "782/782 - 12s - loss: 0.1563 - acc: 0.9399 - val_loss: 0.7232 - val_acc: 0.7659\n",
      "Epoch 45/50\n",
      "782/782 - 12s - loss: 0.1516 - acc: 0.9422 - val_loss: 0.9030 - val_acc: 0.7684\n",
      "Epoch 46/50\n",
      "782/782 - 12s - loss: 0.1419 - acc: 0.9458 - val_loss: 0.7117 - val_acc: 0.7786\n",
      "Epoch 47/50\n",
      "782/782 - 12s - loss: 0.1469 - acc: 0.9451 - val_loss: 0.7207 - val_acc: 0.7839\n",
      "Epoch 48/50\n",
      "782/782 - 12s - loss: 0.1404 - acc: 0.9463 - val_loss: 0.7999 - val_acc: 0.7900\n",
      "Epoch 49/50\n",
      "782/782 - 12s - loss: 0.1424 - acc: 0.9465 - val_loss: 0.7534 - val_acc: 0.7733\n",
      "Epoch 50/50\n",
      "782/782 - 12s - loss: 0.1402 - acc: 0.9463 - val_loss: 0.7234 - val_acc: 0.7754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9a118ad390>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final), verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Training and Test Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 96.90% \n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(np.array(padded), np.array(training_labels_final), verbose=0)\n",
    "print('Training Accuracy: %.2f%% ' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 77.54% \n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(np.array(testing_padded), np.array(testing_labels_final), verbose=0)\n",
    "print('Test Accuracy: %.2f%% ' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Prediction Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing the model for example predictions, the input sentences need to be pre-processed and made ready for the model in the same format as the training data, i.e. tokenized and padded. To evaluate the predictive capability of the trained model, we do the following:\n",
    "\n",
    "Preprocess the records using the same Tokenizer as the training data\n",
    "\n",
    "Use the preprocessed data as the input vectors of the model, and compute the output vectors i.e. the prediction classes and the confidence.\n",
    "\n",
    "In this example, the confidence value is used to set a threshold for performing the classification. If the predicted probability is found to be less than 0.5, we classify the statement as \"negative\", and as \"positive\" if the value is found to be 0.5 or above. We later display that value in percentage, as the <i>confidence</i> value for our model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(sent):\n",
    "    print(\"Input sentence : \" + sent)\n",
    "    sent = np.array([sent])\n",
    "    token_list = loaded_tokenizer.fit_on_texts(sent)\n",
    "    \n",
    "    sequences = loaded_tokenizer.texts_to_sequences(sent)\n",
    "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(sent):\n",
    "    test_example = text_prep(sent)\n",
    "    pred_conf = model.predict(test_example)\n",
    "    pred_class = (model.predict(test_example) > 0.5).astype(\"int32\")\n",
    "    if pred_class[0][0]==0:\n",
    "        sentiment = 'Negative'\n",
    "        conf = 100 - (pred_conf[0][np.argmax(pred_conf)] * 100)\n",
    "    else:\n",
    "        sentiment = 'Positive'\n",
    "        conf = pred_conf[0][np.argmax(pred_conf)] * 100\n",
    "    print(\"%s sentiment; %.2f%% confidence\" % (sentiment, conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : The movie was inspired!\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 16) for input Tensor(\"embedding_input:0\", shape=(None, 16), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 16) for input Tensor(\"embedding_input:0\", shape=(None, 16), dtype=float32), but it was called on an input with incompatible shape (None, 100).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment; 62.69% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"The movie was inspired!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : The movie is a masterpiece and the acting was brilliant\n",
      "Positive sentiment; 99.74% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"The movie is a masterpiece and the acting was brilliant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : The story was convoluted and way too twisted\n",
      "Negative sentiment; 93.67% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"The story was convoluted and way too twisted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : What a waste of time!\n",
      "Negative sentiment; 98.54% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"What a waste of time!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : Nolan is the greatest director Ive known\n",
      "Positive sentiment; 78.33% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"Nolan is the greatest director Ive known\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : Shawshank might be a great movie, but you can't beat Interstellar!\n",
      "Positive sentiment; 99.69% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"Shawshank might be a great movie, but you can't beat Interstellar!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : It was too slow!\n",
      "Negative sentiment; 99.23% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"It was too slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : The Devil, indeed, was in the detail!\n",
      "Negative sentiment; 96.10% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"The Devil, indeed, was in the detail!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : Everybody at the theatre could not wait for it to be over\n",
      "Negative sentiment; 72.72% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"Everybody at the theatre could not wait for it to be over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else.\n",
      "Positive sentiment; 93.74% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.\n",
      "Negative sentiment; 68.11% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : If you haven't seen this movie see it right now!\n",
      "Positive sentiment; 97.22% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"If you haven't seen this movie see it right now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : I went into this not expecting to much but I came out blown away.\n",
      "Positive sentiment; 85.31% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"I went into this not expecting to much but I came out blown away.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence : It is a movie worth seeing due to the fact that the workers put so much time and effort into this one film and they did not want to waste money putting up garbage.\n",
      "Positive sentiment; 91.78% confidence\n"
     ]
    }
   ],
   "source": [
    "predict_sentiment(\"It is a movie worth seeing due to the fact that the workers put so much time and effort into this one film and they did not want to waste money putting up garbage.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
